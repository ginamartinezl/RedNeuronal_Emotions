{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c25733c",
   "metadata": {},
   "source": [
    "Gina Martínez López\n",
    "\n",
    "El objetivo del proyecto es usar una red neuronal para clasificar datos sobre señales de\n",
    "voz, de acuerdo al sentimiento de dicha voz: enojado, triste, feliz.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece13275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar paquetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375568ef",
   "metadata": {},
   "source": [
    "2.\n",
    "Se lee el conjunto de datos y se eliminan los datos y columnas innecesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db545b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>1243</td>\n",
       "      <td>1244</td>\n",
       "      <td>1436</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.254385</td>\n",
       "      <td>0.229653</td>\n",
       "      <td>0.265573</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>2.214752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.202433</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.616536</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>1.398438</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>1244</td>\n",
       "      <td>1245</td>\n",
       "      <td>1437</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.045303</td>\n",
       "      <td>0.248974</td>\n",
       "      <td>0.220745</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>2.474743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.189293</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.115723</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>1245</td>\n",
       "      <td>1246</td>\n",
       "      <td>1438</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.044793</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>2.607668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.070801</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>4.554688</td>\n",
       "      <td>4.289062</td>\n",
       "      <td>0.214936</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>1246</td>\n",
       "      <td>1247</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.082267</td>\n",
       "      <td>0.249435</td>\n",
       "      <td>0.207680</td>\n",
       "      <td>0.268538</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>3.460579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.724888</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>6.539062</td>\n",
       "      <td>0.238857</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>1247</td>\n",
       "      <td>1248</td>\n",
       "      <td>1440</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.245034</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>2.563983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.162938</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.915625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>0.193141</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0     X  meanfreq        sd    median       Q25  \\\n",
       "0               0           1     1  0.181338  0.060495  0.187476  0.126197   \n",
       "1               1           2     2  0.186897  0.062260  0.195070  0.130847   \n",
       "2               2           3     3  0.189102  0.062901  0.204945  0.131422   \n",
       "3               4           5     5  0.183036  0.060051  0.174115  0.129949   \n",
       "4               5           6     6  0.168793  0.057910  0.156266  0.116783   \n",
       "..            ...         ...   ...       ...       ...       ...       ...   \n",
       "904          1243        1244  1436  0.244013  0.035477  0.254385  0.229653   \n",
       "905          1244        1245  1437  0.235383  0.045303  0.248974  0.220745   \n",
       "906          1245        1246  1438  0.231211  0.044793  0.234847  0.221477   \n",
       "907          1246        1247  1439  0.213587  0.082267  0.249435  0.207680   \n",
       "908          1247        1248  1440  0.212537  0.078746  0.245034  0.209794   \n",
       "\n",
       "          Q75       IQR      skew  ...  centroid   meanfun    minfun  \\\n",
       "0    0.233586  0.107389  0.869088  ...  0.181338  0.137742  0.023022   \n",
       "1    0.243987  0.113140  1.191767  ...  0.186897  0.121811  0.018412   \n",
       "2    0.249978  0.118556  1.312690  ...  0.189102  0.123758  0.083333   \n",
       "3    0.236967  0.107017  1.096409  ...  0.183036  0.128469  0.044693   \n",
       "4    0.216326  0.099543  1.386837  ...  0.168793  0.109720  0.022472   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "904  0.265573  0.035920  2.214752  ...  0.244013  0.202433  0.028829   \n",
       "905  0.264233  0.043488  2.474743  ...  0.235383  0.189293  0.031250   \n",
       "906  0.262090  0.040613  2.607668  ...  0.231211  0.171805  0.022346   \n",
       "907  0.268538  0.060858  3.460579  ...  0.213587  0.155277  0.020592   \n",
       "908  0.264031  0.054238  2.563983  ...  0.212537  0.162938  0.024845   \n",
       "\n",
       "       maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0    0.271186  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1    0.271186  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2    0.262295  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3    0.258065  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4    0.235294  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "904  0.271186  0.616536  0.210938  1.609375  1.398438  0.281869  happy  \n",
       "905  0.275862  1.115723  0.265625  5.500000  5.234375  0.167861  happy  \n",
       "906  0.275862  1.070801  0.265625  4.554688  4.289062  0.214936  happy  \n",
       "907  0.275862  1.724888  0.273438  6.812500  6.539062  0.238857  happy  \n",
       "908  0.258065  0.915625  0.000000  6.281250  6.281250  0.193141  happy  \n",
       "\n",
       "[909 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"emotions_by_voice registers.csv\")\n",
    "df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5c4680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>1243</td>\n",
       "      <td>1244</td>\n",
       "      <td>1436</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.254385</td>\n",
       "      <td>0.229653</td>\n",
       "      <td>0.265573</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>2.214752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.202433</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.616536</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>1.398438</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>1244</td>\n",
       "      <td>1245</td>\n",
       "      <td>1437</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.045303</td>\n",
       "      <td>0.248974</td>\n",
       "      <td>0.220745</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>2.474743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.189293</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.115723</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>1245</td>\n",
       "      <td>1246</td>\n",
       "      <td>1438</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.044793</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>2.607668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.070801</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>4.554688</td>\n",
       "      <td>4.289062</td>\n",
       "      <td>0.214936</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>1246</td>\n",
       "      <td>1247</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.082267</td>\n",
       "      <td>0.249435</td>\n",
       "      <td>0.207680</td>\n",
       "      <td>0.268538</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>3.460579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.724888</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>6.539062</td>\n",
       "      <td>0.238857</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>1247</td>\n",
       "      <td>1248</td>\n",
       "      <td>1440</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.245034</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>2.563983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.162938</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.915625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>0.193141</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0     X  meanfreq        sd    median       Q25  \\\n",
       "0               0           1     1  0.181338  0.060495  0.187476  0.126197   \n",
       "1               1           2     2  0.186897  0.062260  0.195070  0.130847   \n",
       "2               2           3     3  0.189102  0.062901  0.204945  0.131422   \n",
       "3               4           5     5  0.183036  0.060051  0.174115  0.129949   \n",
       "4               5           6     6  0.168793  0.057910  0.156266  0.116783   \n",
       "..            ...         ...   ...       ...       ...       ...       ...   \n",
       "904          1243        1244  1436  0.244013  0.035477  0.254385  0.229653   \n",
       "905          1244        1245  1437  0.235383  0.045303  0.248974  0.220745   \n",
       "906          1245        1246  1438  0.231211  0.044793  0.234847  0.221477   \n",
       "907          1246        1247  1439  0.213587  0.082267  0.249435  0.207680   \n",
       "908          1247        1248  1440  0.212537  0.078746  0.245034  0.209794   \n",
       "\n",
       "          Q75       IQR      skew  ...  centroid   meanfun    minfun  \\\n",
       "0    0.233586  0.107389  0.869088  ...  0.181338  0.137742  0.023022   \n",
       "1    0.243987  0.113140  1.191767  ...  0.186897  0.121811  0.018412   \n",
       "2    0.249978  0.118556  1.312690  ...  0.189102  0.123758  0.083333   \n",
       "3    0.236967  0.107017  1.096409  ...  0.183036  0.128469  0.044693   \n",
       "4    0.216326  0.099543  1.386837  ...  0.168793  0.109720  0.022472   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "904  0.265573  0.035920  2.214752  ...  0.244013  0.202433  0.028829   \n",
       "905  0.264233  0.043488  2.474743  ...  0.235383  0.189293  0.031250   \n",
       "906  0.262090  0.040613  2.607668  ...  0.231211  0.171805  0.022346   \n",
       "907  0.268538  0.060858  3.460579  ...  0.213587  0.155277  0.020592   \n",
       "908  0.264031  0.054238  2.563983  ...  0.212537  0.162938  0.024845   \n",
       "\n",
       "       maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0    0.271186  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1    0.271186  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2    0.262295  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3    0.258065  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4    0.235294  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "904  0.271186  0.616536  0.210938  1.609375  1.398438  0.281869  happy  \n",
       "905  0.275862  1.115723  0.265625  5.500000  5.234375  0.167861  happy  \n",
       "906  0.275862  1.070801  0.265625  4.554688  4.289062  0.214936  happy  \n",
       "907  0.275862  1.724888  0.273438  6.812500  6.539062  0.238857  happy  \n",
       "908  0.258065  0.915625  0.000000  6.281250  6.281250  0.193141  happy  \n",
       "\n",
       "[909 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removemos las filas con valores ausentes.\n",
    "df=df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc0a9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>2.863717</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>3.878650</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>4.589995</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>3.680995</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>5.031744</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.254385</td>\n",
       "      <td>0.229653</td>\n",
       "      <td>0.265573</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>2.214752</td>\n",
       "      <td>7.565052</td>\n",
       "      <td>0.821874</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.202433</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.616536</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>1.398438</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.045303</td>\n",
       "      <td>0.248974</td>\n",
       "      <td>0.220745</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>2.474743</td>\n",
       "      <td>9.959019</td>\n",
       "      <td>0.848109</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.189293</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.115723</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.044793</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>2.607668</td>\n",
       "      <td>10.698821</td>\n",
       "      <td>0.848702</td>\n",
       "      <td>0.241998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.070801</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>4.554688</td>\n",
       "      <td>4.289062</td>\n",
       "      <td>0.214936</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.082267</td>\n",
       "      <td>0.249435</td>\n",
       "      <td>0.207680</td>\n",
       "      <td>0.268538</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>3.460579</td>\n",
       "      <td>18.034614</td>\n",
       "      <td>0.882544</td>\n",
       "      <td>0.425394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.724888</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>6.539062</td>\n",
       "      <td>0.238857</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.245034</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>2.563983</td>\n",
       "      <td>10.392885</td>\n",
       "      <td>0.887389</td>\n",
       "      <td>0.404993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.162938</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.915625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>0.193141</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0    0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
       "1    0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
       "2    0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
       "3    0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
       "4    0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "904  0.244013  0.035477  0.254385  0.229653  0.265573  0.035920  2.214752   \n",
       "905  0.235383  0.045303  0.248974  0.220745  0.264233  0.043488  2.474743   \n",
       "906  0.231211  0.044793  0.234847  0.221477  0.262090  0.040613  2.607668   \n",
       "907  0.213587  0.082267  0.249435  0.207680  0.268538  0.060858  3.460579   \n",
       "908  0.212537  0.078746  0.245034  0.209794  0.264031  0.054238  2.563983   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0     2.863717  0.923566  0.307220  ...  0.181338  0.137742  0.023022   \n",
       "1     3.878650  0.918848  0.298859  ...  0.186897  0.121811  0.018412   \n",
       "2     4.589995  0.919519  0.313069  ...  0.189102  0.123758  0.083333   \n",
       "3     3.680995  0.921361  0.329295  ...  0.183036  0.128469  0.044693   \n",
       "4     5.031744  0.926238  0.337047  ...  0.168793  0.109720  0.022472   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "904   7.565052  0.821874  0.136933  ...  0.244013  0.202433  0.028829   \n",
       "905   9.959019  0.848109  0.236957  ...  0.235383  0.189293  0.031250   \n",
       "906  10.698821  0.848702  0.241998  ...  0.231211  0.171805  0.022346   \n",
       "907  18.034614  0.882544  0.425394  ...  0.213587  0.155277  0.020592   \n",
       "908  10.392885  0.887389  0.404993  ...  0.212537  0.162938  0.024845   \n",
       "\n",
       "       maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0    0.271186  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1    0.271186  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2    0.262295  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3    0.258065  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4    0.235294  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "904  0.271186  0.616536  0.210938  1.609375  1.398438  0.281869  happy  \n",
       "905  0.275862  1.115723  0.265625  5.500000  5.234375  0.167861  happy  \n",
       "906  0.275862  1.070801  0.265625  4.554688  4.289062  0.214936  happy  \n",
       "907  0.275862  1.724888  0.273438  6.812500  6.539062  0.238857  happy  \n",
       "908  0.258065  0.915625  0.000000  6.281250  6.281250  0.193141  happy  \n",
       "\n",
       "[909 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remuevo las columnas con datos de indices\n",
    "df=df.drop(df.columns[[0,1,2]], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e200ef6",
   "metadata": {},
   "source": [
    "Se separa la columna objetivo y se crea el conjunto de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dced924d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sad', 'happy', 'angry'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Separar la columna objetivo.\n",
    "X=df.drop('label',axis=1)\n",
    "y=df['label']\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48afd02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=(X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95b04a",
   "metadata": {},
   "source": [
    "Se codifica: happy->0 angry->1 sad->2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7923f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"happy\", \"angry\", \"sad\"])\n",
    "y2= le.transform(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fceb3",
   "metadata": {},
   "source": [
    "El conjunto de datos en atributos X y variable objetivo y. Luego separamos éstos en conjuntos de entrenamiento y testeo, con una proporción de 65-35 para que halla una mejor distribución de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ff12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, random_state = 0, test_size=0.35, train_size=0.65, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5f6e2",
   "metadata": {},
   "source": [
    "Se manejara una validación con proporción 75-25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff243af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validación\n",
    "X_test1, X_val, y_test1, y_val = train_test_split(X_test, y_test, random_state = 0, test_size=0.25, train_size=0.75, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7bf84d",
   "metadata": {},
   "source": [
    "3. Se preparan los datos para alimentar la red neuronal, primero se hace la conversión a tensores de X y y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09af07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    " \n",
    "  def __init__(self,X,y):\n",
    "    self.X=torch.tensor(X.values,dtype=torch.float32)\n",
    "    self.y=torch.tensor(y,dtype=torch.long)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b442e2",
   "metadata": {},
   "source": [
    "Ahora usamos los Dataloaders para los conjuntos set, val y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0bf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la clase MyDataset para preparar cada conjunto en forma de tensores\n",
    "train_sec=MyDataset(X_train, y_train)\n",
    "test_sec=MyDataset(X_test1,y_test1)\n",
    "val_sec=MyDataset(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19586af4",
   "metadata": {},
   "source": [
    "La creación de dataloaders se usaran para proporcionarlos por lotes a la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c228b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los DataLoaders para cargar la información pro lotes\n",
    "train_data=DataLoader(\n",
    "    train_sec,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )\n",
    "\n",
    "test_data=DataLoader(\n",
    "    test_sec,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )\n",
    "\n",
    "val_data=DataLoader(\n",
    "    val_sec,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6156ee",
   "metadata": {},
   "source": [
    "Se imprime el primer bath del Test Set, aqui vemos como con el Dataloader se ingresan los datos a la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b712ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20]) torch.Size([3])\n",
      "tensor([[ 0.0809, -0.9859,  0.1854,  0.3136, -0.6326, -0.7648,  2.0283,  2.2725,\n",
      "         -0.7538, -0.5520,  0.2844,  0.0809,  0.3691,  0.0123, -0.8960,  1.2587,\n",
      "          0.3380,  1.3476,  1.3359,  0.0093],\n",
      "        [-0.1513, -1.0782, -0.1262,  0.1874, -1.2506, -0.9609,  0.9696,  1.1155,\n",
      "         -0.5804, -0.6929, -0.2459, -0.1513, -0.2692, -0.8255, -0.8960,  1.1099,\n",
      "          0.2609,  1.3317,  1.3239,  0.2008],\n",
      "        [ 0.5278, -0.8079,  0.4555,  0.3800, -0.1293, -0.5588, -0.3897, -0.3316,\n",
      "         -0.1240, -0.7063,  0.2304,  0.5278,  0.2858, -0.1065,  0.8857,  0.1165,\n",
      "          0.4150, -0.0903, -0.1122, -0.9317]]) tensor([1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, labels) in enumerate(test_data):\n",
    "  print(data.shape, labels.shape)\n",
    "  print(data,labels)\n",
    "  break;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd47b53",
   "metadata": {},
   "source": [
    "4. \n",
    "Se define la clase Net con la arquitectura de la red neuronal, para este caso se usan 2 capas, sin embargo se realizo el experimento con diferentes tamaños de capas, sin embargo el resultado final era menos optimo, por lo tanto, se decidio dejar 2 capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6dc1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Inicializar las dos capas lineales \n",
    "        self.fc1 = nn.Linear(num_inputs,10)\n",
    "        self.fc2 = nn.Linear(10, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Usar las capas inicializadas y devolver x\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        #x = nn.functional.log_softmax(x,dim=1) #Función de activación\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2fd1c",
   "metadata": {},
   "source": [
    "5. \n",
    "Se define el modelo, el optimizador con Stochastic gradient descent y un Learning rate de 0.1, luego la función de costo denominada como criterion con la función CrossEntropyLoss la cual se usa para los problemas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77e6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net(X_train.shape[1],3)\n",
    "\n",
    "#TODO 5 Definir el optimizador Stochastic gradient descent y la función MeanSquareError. Usar Learnig rate de 0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a00cdd",
   "metadata": {},
   "source": [
    "\n",
    " Se entrena el modelo con un num_epochs de 100 suficiente para poder ver el cambio de los errores de entrenamiento y validación, adicionalmente se imprime el accuracy para identificar si hay algún error, pero se identifica que aumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f639e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenar el modelo\n",
    "\n",
    "def train_model(model,optimizer,loss_module,train_loader,valid_loader,num_epochs=100):\n",
    "  \n",
    "  valid_loss_min =np.inf  #Vamos a encontrar el menor valor de error de validación. Por eso la inicializmaos como 'infinito'\n",
    "  \n",
    "  for i in range(num_epochs):\n",
    "    model.train()  #ponemos el modelo en modo entrenamiento. Es importante en otras arquitecturas como redes convolucionales.\n",
    "    train_loss = 0.0\n",
    "    v_loss = 0.0\n",
    "    train_accuracy=0.0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "\n",
    "        # Reiniciar los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: calcular la salida para los datos de entrada..\n",
    "        pred=model(data)\n",
    "        # calculate the batch loss\n",
    "        loss=loss_module(pred,target)\n",
    "        # backpropagation: cálculo de gradientes\n",
    "        loss.backward()\n",
    "        # actualizar los parámetros\n",
    "        optimizer.step()\n",
    "        # actualizar la cuenta de costos a lo largo de los lotes\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "\n",
    "\n",
    "        total += target.size(0)\n",
    "        train_accuracy += (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    train_accuracy = train_accuracy / total\n",
    "    train_loss = train_loss/len(train_loader.dataset) \n",
    "\n",
    "\n",
    "    model.eval() #Ponemos el modelo en modo evaluación.\n",
    "\n",
    "    # vamos a evaluar el modelo entrenado, calculando predicciones con el conjunto de validación\n",
    "    \n",
    "    for data,target in valid_loader:\n",
    "      output=model(data)\n",
    "      valid_loss= loss_module(output,target)\n",
    "      valid_loss += loss.item()*data.size(0)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    #imprimir estadísticas de entrenamiento, validación y accuracy\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t train accuracy: {:.6f}'.format(\n",
    "        i, train_loss, valid_loss, train_accuracy))\n",
    "    \n",
    "    #Guardamos el modelo con el menor error de validación.\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'mejor_modelo.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b864f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.050604 \tValidation Loss: 0.038475 \t train accuracy: 0.452542\n",
      "Validation loss decreased (inf --> 0.038475).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 0.956086 \tValidation Loss: 0.047805 \t train accuracy: 0.544068\n",
      "Epoch: 2 \tTraining Loss: 0.931357 \tValidation Loss: 0.053040 \t train accuracy: 0.555932\n",
      "Epoch: 3 \tTraining Loss: 0.917564 \tValidation Loss: 0.056610 \t train accuracy: 0.572881\n",
      "Epoch: 4 \tTraining Loss: 0.908560 \tValidation Loss: 0.058888 \t train accuracy: 0.584746\n",
      "Epoch: 5 \tTraining Loss: 0.901992 \tValidation Loss: 0.060205 \t train accuracy: 0.589831\n",
      "Epoch: 6 \tTraining Loss: 0.896910 \tValidation Loss: 0.060887 \t train accuracy: 0.586441\n",
      "Epoch: 7 \tTraining Loss: 0.892852 \tValidation Loss: 0.061167 \t train accuracy: 0.581356\n",
      "Epoch: 8 \tTraining Loss: 0.889537 \tValidation Loss: 0.061194 \t train accuracy: 0.579661\n",
      "Epoch: 9 \tTraining Loss: 0.886775 \tValidation Loss: 0.061065 \t train accuracy: 0.577966\n",
      "Epoch: 10 \tTraining Loss: 0.884430 \tValidation Loss: 0.060844 \t train accuracy: 0.583051\n",
      "Epoch: 11 \tTraining Loss: 0.882406 \tValidation Loss: 0.060574 \t train accuracy: 0.586441\n",
      "Epoch: 12 \tTraining Loss: 0.880633 \tValidation Loss: 0.060280 \t train accuracy: 0.584746\n",
      "Epoch: 13 \tTraining Loss: 0.879061 \tValidation Loss: 0.059982 \t train accuracy: 0.591525\n",
      "Epoch: 14 \tTraining Loss: 0.877652 \tValidation Loss: 0.059690 \t train accuracy: 0.593220\n",
      "Epoch: 15 \tTraining Loss: 0.876379 \tValidation Loss: 0.059410 \t train accuracy: 0.591525\n",
      "Epoch: 16 \tTraining Loss: 0.875221 \tValidation Loss: 0.059147 \t train accuracy: 0.594915\n",
      "Epoch: 17 \tTraining Loss: 0.874162 \tValidation Loss: 0.058900 \t train accuracy: 0.594915\n",
      "Epoch: 18 \tTraining Loss: 0.873189 \tValidation Loss: 0.058671 \t train accuracy: 0.594915\n",
      "Epoch: 19 \tTraining Loss: 0.872291 \tValidation Loss: 0.058458 \t train accuracy: 0.594915\n",
      "Epoch: 20 \tTraining Loss: 0.871460 \tValidation Loss: 0.058261 \t train accuracy: 0.594915\n",
      "Epoch: 21 \tTraining Loss: 0.870689 \tValidation Loss: 0.058078 \t train accuracy: 0.594915\n",
      "Epoch: 22 \tTraining Loss: 0.869972 \tValidation Loss: 0.057908 \t train accuracy: 0.594915\n",
      "Epoch: 23 \tTraining Loss: 0.869302 \tValidation Loss: 0.057750 \t train accuracy: 0.591525\n",
      "Epoch: 24 \tTraining Loss: 0.868677 \tValidation Loss: 0.057602 \t train accuracy: 0.591525\n",
      "Epoch: 25 \tTraining Loss: 0.868091 \tValidation Loss: 0.057463 \t train accuracy: 0.591525\n",
      "Epoch: 26 \tTraining Loss: 0.867541 \tValidation Loss: 0.057333 \t train accuracy: 0.591525\n",
      "Epoch: 27 \tTraining Loss: 0.867024 \tValidation Loss: 0.057210 \t train accuracy: 0.589831\n",
      "Epoch: 28 \tTraining Loss: 0.866537 \tValidation Loss: 0.057094 \t train accuracy: 0.589831\n",
      "Epoch: 29 \tTraining Loss: 0.866078 \tValidation Loss: 0.056984 \t train accuracy: 0.591525\n",
      "Epoch: 30 \tTraining Loss: 0.865644 \tValidation Loss: 0.056880 \t train accuracy: 0.593220\n",
      "Epoch: 31 \tTraining Loss: 0.865233 \tValidation Loss: 0.056780 \t train accuracy: 0.591525\n",
      "Epoch: 32 \tTraining Loss: 0.864844 \tValidation Loss: 0.056685 \t train accuracy: 0.591525\n",
      "Epoch: 33 \tTraining Loss: 0.864475 \tValidation Loss: 0.056594 \t train accuracy: 0.591525\n",
      "Epoch: 34 \tTraining Loss: 0.864124 \tValidation Loss: 0.056508 \t train accuracy: 0.591525\n",
      "Epoch: 35 \tTraining Loss: 0.863790 \tValidation Loss: 0.056424 \t train accuracy: 0.591525\n",
      "Epoch: 36 \tTraining Loss: 0.863473 \tValidation Loss: 0.056344 \t train accuracy: 0.591525\n",
      "Epoch: 37 \tTraining Loss: 0.863169 \tValidation Loss: 0.056267 \t train accuracy: 0.591525\n",
      "Epoch: 38 \tTraining Loss: 0.862880 \tValidation Loss: 0.056192 \t train accuracy: 0.589831\n",
      "Epoch: 39 \tTraining Loss: 0.862603 \tValidation Loss: 0.056120 \t train accuracy: 0.589831\n",
      "Epoch: 40 \tTraining Loss: 0.862339 \tValidation Loss: 0.056051 \t train accuracy: 0.589831\n",
      "Epoch: 41 \tTraining Loss: 0.862085 \tValidation Loss: 0.055983 \t train accuracy: 0.589831\n",
      "Epoch: 42 \tTraining Loss: 0.861842 \tValidation Loss: 0.055918 \t train accuracy: 0.591525\n",
      "Epoch: 43 \tTraining Loss: 0.861609 \tValidation Loss: 0.055855 \t train accuracy: 0.596610\n",
      "Epoch: 44 \tTraining Loss: 0.861385 \tValidation Loss: 0.055794 \t train accuracy: 0.596610\n",
      "Epoch: 45 \tTraining Loss: 0.861170 \tValidation Loss: 0.055734 \t train accuracy: 0.594915\n",
      "Epoch: 46 \tTraining Loss: 0.860962 \tValidation Loss: 0.055677 \t train accuracy: 0.594915\n",
      "Epoch: 47 \tTraining Loss: 0.860763 \tValidation Loss: 0.055621 \t train accuracy: 0.594915\n",
      "Epoch: 48 \tTraining Loss: 0.860571 \tValidation Loss: 0.055566 \t train accuracy: 0.593220\n",
      "Epoch: 49 \tTraining Loss: 0.860386 \tValidation Loss: 0.055513 \t train accuracy: 0.593220\n",
      "Epoch: 50 \tTraining Loss: 0.860207 \tValidation Loss: 0.055461 \t train accuracy: 0.593220\n",
      "Epoch: 51 \tTraining Loss: 0.860035 \tValidation Loss: 0.055411 \t train accuracy: 0.591525\n",
      "Epoch: 52 \tTraining Loss: 0.859868 \tValidation Loss: 0.055361 \t train accuracy: 0.591525\n",
      "Epoch: 53 \tTraining Loss: 0.859707 \tValidation Loss: 0.055313 \t train accuracy: 0.591525\n",
      "Epoch: 54 \tTraining Loss: 0.859551 \tValidation Loss: 0.055267 \t train accuracy: 0.591525\n",
      "Epoch: 55 \tTraining Loss: 0.859401 \tValidation Loss: 0.055221 \t train accuracy: 0.591525\n",
      "Epoch: 56 \tTraining Loss: 0.859255 \tValidation Loss: 0.055176 \t train accuracy: 0.591525\n",
      "Epoch: 57 \tTraining Loss: 0.859114 \tValidation Loss: 0.055133 \t train accuracy: 0.591525\n",
      "Epoch: 58 \tTraining Loss: 0.858977 \tValidation Loss: 0.055090 \t train accuracy: 0.591525\n",
      "Epoch: 59 \tTraining Loss: 0.858844 \tValidation Loss: 0.055048 \t train accuracy: 0.591525\n",
      "Epoch: 60 \tTraining Loss: 0.858716 \tValidation Loss: 0.055008 \t train accuracy: 0.591525\n",
      "Epoch: 61 \tTraining Loss: 0.858591 \tValidation Loss: 0.054968 \t train accuracy: 0.591525\n",
      "Epoch: 62 \tTraining Loss: 0.858470 \tValidation Loss: 0.054928 \t train accuracy: 0.591525\n",
      "Epoch: 63 \tTraining Loss: 0.858353 \tValidation Loss: 0.054890 \t train accuracy: 0.591525\n",
      "Epoch: 64 \tTraining Loss: 0.858239 \tValidation Loss: 0.054853 \t train accuracy: 0.589831\n",
      "Epoch: 65 \tTraining Loss: 0.858128 \tValidation Loss: 0.054816 \t train accuracy: 0.591525\n",
      "Epoch: 66 \tTraining Loss: 0.858020 \tValidation Loss: 0.054780 \t train accuracy: 0.591525\n",
      "Epoch: 67 \tTraining Loss: 0.857916 \tValidation Loss: 0.054744 \t train accuracy: 0.591525\n",
      "Epoch: 68 \tTraining Loss: 0.857814 \tValidation Loss: 0.054710 \t train accuracy: 0.591525\n",
      "Epoch: 69 \tTraining Loss: 0.857715 \tValidation Loss: 0.054675 \t train accuracy: 0.591525\n",
      "Epoch: 70 \tTraining Loss: 0.857619 \tValidation Loss: 0.054642 \t train accuracy: 0.591525\n",
      "Epoch: 71 \tTraining Loss: 0.857526 \tValidation Loss: 0.054609 \t train accuracy: 0.591525\n",
      "Epoch: 72 \tTraining Loss: 0.857434 \tValidation Loss: 0.054577 \t train accuracy: 0.591525\n",
      "Epoch: 73 \tTraining Loss: 0.857346 \tValidation Loss: 0.054545 \t train accuracy: 0.589831\n",
      "Epoch: 74 \tTraining Loss: 0.857259 \tValidation Loss: 0.054513 \t train accuracy: 0.588136\n",
      "Epoch: 75 \tTraining Loss: 0.857175 \tValidation Loss: 0.054483 \t train accuracy: 0.588136\n",
      "Epoch: 76 \tTraining Loss: 0.857093 \tValidation Loss: 0.054452 \t train accuracy: 0.588136\n",
      "Epoch: 77 \tTraining Loss: 0.857014 \tValidation Loss: 0.054422 \t train accuracy: 0.588136\n",
      "Epoch: 78 \tTraining Loss: 0.856936 \tValidation Loss: 0.054393 \t train accuracy: 0.588136\n",
      "Epoch: 79 \tTraining Loss: 0.856860 \tValidation Loss: 0.054364 \t train accuracy: 0.588136\n",
      "Epoch: 80 \tTraining Loss: 0.856786 \tValidation Loss: 0.054335 \t train accuracy: 0.589831\n",
      "Epoch: 81 \tTraining Loss: 0.856714 \tValidation Loss: 0.054307 \t train accuracy: 0.589831\n",
      "Epoch: 82 \tTraining Loss: 0.856643 \tValidation Loss: 0.054280 \t train accuracy: 0.589831\n",
      "Epoch: 83 \tTraining Loss: 0.856574 \tValidation Loss: 0.054252 \t train accuracy: 0.589831\n",
      "Epoch: 84 \tTraining Loss: 0.856507 \tValidation Loss: 0.054225 \t train accuracy: 0.589831\n",
      "Epoch: 85 \tTraining Loss: 0.856442 \tValidation Loss: 0.054198 \t train accuracy: 0.589831\n",
      "Epoch: 86 \tTraining Loss: 0.856378 \tValidation Loss: 0.054172 \t train accuracy: 0.589831\n",
      "Epoch: 87 \tTraining Loss: 0.856315 \tValidation Loss: 0.054146 \t train accuracy: 0.589831\n",
      "Epoch: 88 \tTraining Loss: 0.856254 \tValidation Loss: 0.054120 \t train accuracy: 0.589831\n",
      "Epoch: 89 \tTraining Loss: 0.856194 \tValidation Loss: 0.054095 \t train accuracy: 0.589831\n",
      "Epoch: 90 \tTraining Loss: 0.856136 \tValidation Loss: 0.054070 \t train accuracy: 0.589831\n",
      "Epoch: 91 \tTraining Loss: 0.856079 \tValidation Loss: 0.054045 \t train accuracy: 0.589831\n",
      "Epoch: 92 \tTraining Loss: 0.856023 \tValidation Loss: 0.054021 \t train accuracy: 0.589831\n",
      "Epoch: 93 \tTraining Loss: 0.855968 \tValidation Loss: 0.053996 \t train accuracy: 0.589831\n",
      "Epoch: 94 \tTraining Loss: 0.855915 \tValidation Loss: 0.053972 \t train accuracy: 0.589831\n",
      "Epoch: 95 \tTraining Loss: 0.855862 \tValidation Loss: 0.053949 \t train accuracy: 0.589831\n",
      "Epoch: 96 \tTraining Loss: 0.855811 \tValidation Loss: 0.053925 \t train accuracy: 0.589831\n",
      "Epoch: 97 \tTraining Loss: 0.855761 \tValidation Loss: 0.053902 \t train accuracy: 0.589831\n",
      "Epoch: 98 \tTraining Loss: 0.855711 \tValidation Loss: 0.053879 \t train accuracy: 0.589831\n",
      "Epoch: 99 \tTraining Loss: 0.855663 \tValidation Loss: 0.053856 \t train accuracy: 0.589831\n"
     ]
    }
   ],
   "source": [
    "train_model(model,optimizer,criterion,train_data,val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c8120",
   "metadata": {},
   "source": [
    "6.\n",
    "Se imprimen los parámetros del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2208c71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mejor_modelo.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae0a96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight tensor([[-2.0780e-01, -7.0553e-02, -2.6968e-01, -2.2490e-01,  3.6765e-02,\n",
      "         -7.6568e-02, -8.0756e-02, -2.2143e-02, -7.0459e-02,  1.4234e-01,\n",
      "          9.1919e-02, -1.2659e-01, -6.6352e-02,  2.4918e-01,  1.2892e-01,\n",
      "         -8.5321e-02,  1.3112e-01,  2.3676e-01, -1.4436e-01, -5.7857e-02],\n",
      "        [ 3.9667e-02,  2.4754e-02, -1.2540e-01,  1.7153e-02,  8.6064e-02,\n",
      "         -2.1354e-01, -8.3746e-02,  1.0602e-01,  3.0075e-02, -1.6061e-01,\n",
      "          3.2718e-02, -6.7988e-02, -2.3838e-02,  2.8118e-01, -1.2420e-01,\n",
      "          2.4512e-01, -1.6226e-01, -1.5698e-01,  2.3613e-01, -4.0414e-02],\n",
      "        [ 8.2673e-02,  5.0848e-02,  9.0254e-02,  1.1390e-01, -1.6175e-01,\n",
      "         -1.4797e-01,  8.8444e-02, -8.6820e-02, -3.5701e-04, -1.4655e-01,\n",
      "          5.7093e-02,  2.0184e-01, -9.4009e-02,  1.4178e-01,  5.6044e-02,\n",
      "         -8.4096e-02, -3.8238e-02, -1.7054e-01, -1.5683e-01, -2.1845e-01],\n",
      "        [-2.4519e-02,  7.1004e-02, -1.4392e-01,  6.8924e-02,  7.9285e-02,\n",
      "         -2.4765e-02,  1.2605e-02,  2.8036e-02, -3.1085e-02,  2.3123e-01,\n",
      "          2.7879e-01,  7.4835e-02, -7.0730e-02,  1.8387e-01,  1.4374e-01,\n",
      "         -2.5629e-02,  3.2876e-02, -1.7041e-01, -1.2579e-01, -2.9796e-02],\n",
      "        [ 7.3610e-02,  1.4900e-01,  1.4979e-01,  2.8892e-02,  9.5764e-02,\n",
      "          3.4368e-02, -4.4118e-02, -2.5554e-01,  1.0294e-01,  1.7019e-01,\n",
      "          9.6975e-02,  9.4680e-02,  9.8260e-02, -2.1568e-01,  1.1226e-01,\n",
      "          2.0029e-01,  2.2778e-01,  5.2456e-02,  7.4090e-02,  1.1668e-01],\n",
      "        [ 3.3112e-02, -2.4002e-01,  9.1931e-02,  1.2715e-01,  1.1330e-01,\n",
      "          1.6771e-01,  9.1646e-03,  2.6421e-01,  2.7187e-02,  3.9651e-02,\n",
      "         -2.1632e-01,  2.1222e-02, -1.1518e-01, -3.0913e-02,  1.5653e-01,\n",
      "         -1.0213e-02,  1.2043e-01,  1.0365e-01,  1.1629e-01, -1.2026e-01],\n",
      "        [-7.2164e-02,  2.3544e-01,  2.4519e-01,  6.6678e-02,  4.1697e-02,\n",
      "         -2.0442e-01,  1.4615e-01,  1.5516e-01,  2.2508e-01, -1.1325e-01,\n",
      "          2.8234e-01, -9.8146e-03, -3.8200e-02,  1.0021e-01,  2.1766e-01,\n",
      "          8.2521e-02, -1.2372e-01,  1.6660e-01,  1.6139e-01,  1.4993e-01],\n",
      "        [-1.7960e-01, -2.6058e-02,  4.2887e-02,  1.6896e-01, -4.3235e-02,\n",
      "         -2.8113e-03, -3.2959e-03,  6.8389e-02,  2.2872e-02, -2.3719e-01,\n",
      "         -1.8067e-02, -1.6140e-03, -3.8902e-02,  9.4731e-02,  6.3820e-02,\n",
      "         -7.8236e-02,  1.5579e-02,  1.9481e-01,  5.0878e-02,  1.9030e-01],\n",
      "        [ 1.5508e-01,  6.1159e-02,  1.2827e-01, -7.8063e-03,  1.4446e-01,\n",
      "         -9.2384e-02,  1.6791e-01,  1.2025e-01,  1.0568e-02, -9.5944e-02,\n",
      "         -5.3965e-02, -8.0528e-02, -1.6240e-01,  1.4616e-01,  4.2900e-02,\n",
      "         -1.6656e-01, -1.2094e-01, -4.6469e-03, -1.3810e-01,  1.8662e-01],\n",
      "        [-8.4326e-02,  8.5651e-02,  1.2528e-01, -8.3250e-05, -2.7784e-02,\n",
      "          1.1801e-01,  1.8527e-02, -1.9152e-01, -1.5697e-01,  1.7222e-01,\n",
      "          1.5191e-01,  1.4001e-01, -1.1641e-01,  7.7653e-02, -4.6074e-02,\n",
      "         -1.5770e-02,  4.5018e-02, -2.4354e-01, -7.9020e-02,  1.5438e-01]])\n",
      "fc1.bias tensor([-0.0575,  0.0571,  0.0971,  0.0273,  0.0496, -0.0954,  0.0064,  0.1438,\n",
      "        -0.2534,  0.0701])\n",
      "fc2.weight tensor([[ 0.0954,  0.2531, -0.0974,  0.0985,  0.1719,  0.0686,  0.1768, -0.1131,\n",
      "          0.0190,  0.0175],\n",
      "        [-0.2901, -0.2690,  0.2571,  0.0872,  0.2517, -0.1975,  0.0544,  0.0113,\n",
      "          0.2829,  0.1670],\n",
      "        [ 0.1442, -0.0104,  0.1546, -0.1446, -0.3900,  0.1649, -0.2811,  0.0269,\n",
      "          0.0858, -0.1632]])\n",
      "fc2.bias tensor([-0.1352, -0.1218,  0.0497])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947587c",
   "metadata": {},
   "source": [
    "7.\n",
    "Se evalua el desempeño del mejor modelo, para esto se define testAccuracy el cual nos da el accuracy y aqui es donde nos indica si el modelo predice bien los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25780b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy():\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_data:\n",
    "            pred, target = data\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(pred)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)            \n",
    "            total += target.size(0)\n",
    "            accuracy += (predicted == target).sum().item()\n",
    "               \n",
    "    # compute the accuracy over all test \n",
    "    accuracy = (100 * accuracy / total)\n",
    "    print(\"Accuracy of the model:\", accuracy)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a9e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 56.90376569037657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56.90376569037657"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ad82e",
   "metadata": {},
   "source": [
    "El accuracy esta por encima del 50% lo cual es un buen indicador para un clasificador de 3 clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec19b49",
   "metadata": {},
   "source": [
    "8. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadf2f6",
   "metadata": {},
   "source": [
    "El modelo indica que predice de forma correcta mas del 50% de los datos, sin embargo no predice en su totalidad los datos que se muestren, para este proyecto se manejaron 2 capas y se normalizaron los datos. Para mejorarlo se tendria que pensar en una arquitectura mas compleja que no sean solo capas lineales, se realizo el experimento con distinto tamaños y cantidad de capaz y al mantener capas lineales y al aumentar el tamaño de estas se redujo el accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a932c8a",
   "metadata": {},
   "source": [
    "9. \n",
    "Se elije aleatoriamente un registro del dataset, para esto el modelo recibe un tensor X, por esta razón se genera el registro aleatorio apartir de X y se define como float32 para que no entre en conflicto al momento de ponerlo en model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "153bb0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3937, -0.8742,  0.3832,  0.1757, -0.0188, -0.2348, -0.9784, -0.9445,\n",
       "        -0.0447, -0.8407,  0.2619,  0.3937,  0.2718, -0.5323, -0.0626,  0.7035,\n",
       "         0.4920,  0.8868,  0.8651,  0.4406])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "reg_aleatorio = torch.tensor(X.values[randint(0,len(X)-1)], dtype= torch.float32)\n",
    "reg_aleatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b62325",
   "metadata": {},
   "source": [
    "La función sigmoid muestra la probabilidad entre las 3 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebf09080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4734, 0.4497, 0.4305], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(model(reg_aleatorio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a0393",
   "metadata": {},
   "source": [
    "Se muestra la probabilidad entre indices del tensor 0,1,2 siendo happy->0  angry->1  sad->2 para así predecir su sentimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a150716164569798468a1eaa031fb2bd1c5363a0c0d9d75c933ba80c06efdad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
